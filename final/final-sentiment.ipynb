{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ тональности отзывов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала возьмем выборку отзывов на фильмы из NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import numpy as np\n",
    "\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# для сохранения ответов\n",
    "def saveAnswer(data, name):\n",
    "    with open(name, 'w') as file:\n",
    "        file.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приготовим список текстов и классов как обучающую выборку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negfeats = [\" \".join(movie_reviews.words(fileids=[f])) for f in negids]\n",
    "posfeats = [\" \".join(movie_reviews.words(fileids=[f])) for f in posids]\n",
    "\n",
    "texts = negfeats + posfeats\n",
    "labels = [0] * len(negfeats) + [1] * len(posfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "2000\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print len(negfeats)\n",
    "print len(posfeats)\n",
    "print len(texts)\n",
    "\n",
    "saveAnswer(str(len(texts)), 'finalAnswer1.txt')\n",
    "\n",
    "rez = len(posfeats)/(len(texts)*1.)\n",
    "saveAnswer(str(rez), 'finalAnswer2.txt')\n",
    "print rez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем нужные нам модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spiker/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Оценка качества работы разных классификаторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "matrix = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 39659)\n"
     ]
    }
   ],
   "source": [
    "print matrix.shape\n",
    "saveAnswer(str(matrix.shape[1]), 'finalAnswer3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.836021650393\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(\n",
    "            [(\"vectorizer\", vectorizer),\n",
    "            (\"classifier\", LogisticRegression())]\n",
    "        )\n",
    "\n",
    "score = cross_val_score(pipe, texts, labels, scoring='accuracy').mean()\n",
    "print score\n",
    "saveAnswer(str(score), 'finalAnswer4.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.910776493783\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(pipe, texts, labels, scoring='roc_auc').mean()\n",
    "print score\n",
    "saveAnswer(str(score), 'finalAnswer5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spiker/anaconda/lib/python2.7/site-packages/sklearn/utils/deprecation.py:70: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "fit = pipe.fit_transform(texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = pipe.get_params()['classifier']\n",
    "vct = pipe.get_params()['vectorizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coef = clf.coef_[0]\n",
    "coefSort = sorted(range(len(coef)), key=lambda k: coef[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.78217635600981117, -0.63661880890828315, -0.59290172592717927, -0.50817891600105913, -0.50398874258300197]\n",
      "[u'bad', u'unfortunately', u'worst', u'waste', u'nothing']\n"
     ]
    }
   ],
   "source": [
    "print [coef[val] for val in coefSort[:5]]\n",
    "features = vct.get_feature_names()\n",
    "attrs = [features[val] for val in coefSort[:5]]\n",
    "print attrs\n",
    "\n",
    "saveAnswer(' '.join(attrs[:2]), 'finalAnswer6.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#2 week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_classifier(vectorizer, classifier):\n",
    "    return Pipeline(\n",
    "            [(\"vectorizer\", vectorizer),\n",
    "            (\"classifier\", classifier)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'> 0.841   0.0167779617356 \n",
      "\n",
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'> 0.821   0.00406201920232 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Проведем оценку среднего качества и стандартное отклонение с векторизаторами CountVectorizer и TfidfVectorizer\n",
    "rez = np.array([])\n",
    "for vect in [CountVectorizer, TfidfVectorizer]:\n",
    "    scores = cross_val_score(text_classifier(vect(), LogisticRegression()), texts, labels, cv=5)\n",
    "    print vect,scores.mean(),' ', scores.std(),'\\n'\n",
    "    rez = np.append(rez, scores.mean())\n",
    "    rez = np.append(rez, scores.std())\n",
    "\n",
    "attrs = [str(val) for val in rez]\n",
    "saveAnswer(' '.join(attrs), '2-finalAnswer1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_df= 10   0.839   0.0118953772534 \n",
      "\n",
      "min_df= 50   0.813   0.0134536240471 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Проведем оценку среднего качества и стандартное отклонение у CountVectorizer с разными min_df\n",
    "rez = np.array([])\n",
    "for min_df in [10, 50]:\n",
    "    scores = cross_val_score(text_classifier(CountVectorizer(min_df=min_df), LogisticRegression()), texts, labels, cv=5)\n",
    "    print 'min_df=',min_df,' ',scores.mean(),' ', scores.std(),'\\n'\n",
    "    rez = np.append(rez, scores.mean())\n",
    "\n",
    "attrs = [str(val) for val in rez]\n",
    "saveAnswer(' '.join(attrs), '2-finalAnswer2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.logistic.LogisticRegression'> 0.836021650393\n",
      "<class 'sklearn.svm.classes.LinearSVC'> 0.827517637398\n",
      "<class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> 0.765464566363\n",
      "minScore 0.765464566363\n"
     ]
    }
   ],
   "source": [
    "# Проведем оценку разных классификаторов с CountVectorizer\n",
    "minScore = -1\n",
    "for clf in [LogisticRegression, LinearSVC, SGDClassifier]:\n",
    "    score = cross_val_score(text_classifier(CountVectorizer(), clf()), texts, labels).mean()\n",
    "    print clf,score\n",
    "    if minScore == -1 or minScore > score:\n",
    "        minScore = score\n",
    "        \n",
    "print 'minScore', minScore        \n",
    "saveAnswer(str(minScore), '2-finalAnswer3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8415   0.0104403065089 \n",
      "\n",
      "0.839   0.00982344135219 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Проверим качество векторизатора с разными наборами стоп слов\n",
    "rez = np.array([])\n",
    "for stopW in [nltk.corpus.stopwords.words('english'), 'english']:\n",
    "    scores = cross_val_score(text_classifier(CountVectorizer(stop_words=stopW), LogisticRegression()), texts, labels, cv=5)\n",
    "    print scores.mean(),' ', scores.std(),'\\n'\n",
    "    rez = np.append(rez, scores.mean())\n",
    "\n",
    "attrs = [str(val) for val in rez]\n",
    "saveAnswer(' '.join(attrs), '2-finalAnswer4.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8525   0.0165075740192 \n",
      "\n",
      "0.82   0.0106066017178 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Проверим качество векторизатора с разными биграммами\n",
    "scoresWord = cross_val_score(text_classifier(CountVectorizer(ngram_range=(1, 2)), LogisticRegression()), texts, labels, cv=5)\n",
    "print scoresWord.mean(),' ', scoresWord.std(),'\\n'\n",
    "scoresChar = cross_val_score(text_classifier(CountVectorizer(ngram_range=(3, 5), analyzer='char_wb'), LogisticRegression()), texts, labels, cv=5)\n",
    "print scoresChar.mean(),' ', scoresChar.std(),'\\n'\n",
    "\n",
    "attrs = [str(val) for val in [scoresWord.mean(), scoresChar.mean()]]\n",
    "saveAnswer(' '.join(attrs), '2-finalAnswer5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
